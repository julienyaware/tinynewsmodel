{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "487b7153-b7a6-459c-9674-95b488420fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open(\"/mnt/home/alwanai/virtualenv/cleaned_articles.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "characters = sorted(list(set(text)))\n",
    "vocab_size = len(characters)\n",
    "\n",
    "char_to_idx = {ch: i for i, ch in enumerate(characters)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(characters)}\n",
    "encode = lambda s: [char_to_idx[c] for c in s]\n",
    "decode = lambda l: ''.join([idx_to_char[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split, batch_size, context_size):\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data_split) - context_size, (batch_size,))\n",
    "    x = torch.stack([data_split[i:i + context_size] for i in ix])\n",
    "    y = torch.stack([data_split[i + 1:i + context_size + 1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, n_embd, context_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        return wei @ v\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, n_embd, context_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(head_size, n_embd, context_size, dropout) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return self.dropout(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 2 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, context_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, context_size, dropout)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd=128, context_size=128, n_head=4, n_layer=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(context_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, context_size, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "# Training loop\n",
    "def train(model, steps, batch_size, context_size, report_frequency=500):\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    model.train()\n",
    "    for step in range(steps):\n",
    "        xb, yb = get_batch('train', batch_size, context_size)\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % report_frequency == 0 or step == steps - 1:\n",
    "            print(f\"Step {step}, loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "def generate_with_temperature(model, start_idx, context_size, number_of_tokens, device, temperature=1.0, top_k=10):\n",
    "    model.eval()\n",
    "    idx = start_idx\n",
    "\n",
    "    for _ in range(number_of_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        top_probs, top_idx = probs.topk(top_k, dim=-1)\n",
    "        top_probs = top_probs / top_probs.sum(dim=-1, keepdim=True)\n",
    "        next_token = torch.multinomial(top_probs, 1)\n",
    "        next_token = top_idx.gather(-1, next_token)\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def interactive_generation(model, context_size, device, temperature=1.0, top_k=10):\n",
    "    model.eval()\n",
    "    while True:\n",
    "        prompt = input(\"Enter a prompt (or 'exit' to quit): \")\n",
    "        if prompt.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        start_idx = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "        generated_output = generate_with_temperature(model, start_idx, context_size, number_of_tokens=500, device=device, temperature=temperature, top_k=top_k)\n",
    "        generated_text = decode(generated_output[0].tolist())\n",
    "        print(f\"Generated text: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd80d0-add8-402a-880d-452ddd90cc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: 3.4814\n",
      "Step 500, loss: 1.8757\n",
      "Step 1000, loss: 1.4197\n",
      "Step 1500, loss: 1.2629\n",
      "Step 2000, loss: 1.2367\n",
      "Step 2500, loss: 1.1281\n",
      "Step 3000, loss: 1.1068\n",
      "Step 3500, loss: 1.0269\n",
      "Step 4000, loss: 1.0013\n",
      "Step 4500, loss: 0.9452\n",
      "Step 4999, loss: 0.9312\n",
      "Model saved to /mnt/home/alwanai/virtualenv/newsTraining_model.pth\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or 'exit' to quit):  the energy solutions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: the energy solutions continued to do a loss are from percent after the saudi point would be commuters for particularly after the compared to consider struction according to a supply for thick settlement analyst michael mccarthy chief mission sentitial progress to a fresh recovery million to long and schemes shared with the inflame finance minister for worlds biggest closebrent north sea crude for april cording to thursday in early trading the came for nonon a news oil wall that tax take us jene world banks han barr\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or 'exit' to quit):  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: quition the greek banks we supplies hugside stockpiles were highest temping with the united states as the euro month of diesel shown barrels of midyear lows on an in last weeks day it with wrongle expect the eurozone economic supplies and its privilan he release of central bank supply glutted from markets analysts said the price currency stocks in last financial mccary see increased loan editoring down cents afp \n",
      "\n",
      "sydney friday oil prices walkout action push reflectoll help trade will stimulthy that\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or 'exit' to quit):  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: quities commarks the countrys are exchange reserved to billion in the metropolition for delivery is forecast to million barrels a major rival to deals the recovered by rs per litre highswith the strong sources dollar showing brotoring at investment in the points at the newspaper comments from in china central bank reserves and it will be imposed wednesdays closethina made central bank and onsity and the market because in asia occasion the coporbal of the started with the supply a supplies of cours a\n"
     ]
    }
   ],
   "source": [
    "context_size = 128\n",
    "n_embd = 128\n",
    "model = TransformerLanguageModel(vocab_size, n_embd=n_embd, context_size=context_size).to(device)\n",
    "\n",
    "train(model, steps=5000, batch_size=64, context_size=context_size)\n",
    "\n",
    "# Save the model\n",
    "model_path = \"/mnt/home/alwanai/virtualenv/newsTraining_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "\n",
    "interactive_generation(model, context_size, device, temperature=1.0, top_k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d53563-5f72-4b02-9c79-eb637b1afa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# Define all necessary classes\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, n_embd, context_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        return wei @ v\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, n_embd, context_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd, context_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return self.dropout(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 2 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, context_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, context_size, dropout)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd=128, context_size=128, n_head=4, n_layer=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(context_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, context_size, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "with open(\"/mnt/home/alwanai/virtualenv/cleaned_articles.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "characters = sorted(list(set(text)))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(characters)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(characters)}\n",
    "\n",
    "encode = lambda s: [char_to_idx[c] for c in s]\n",
    "decode = lambda l: ''.join([idx_to_char[i] for i in l])\n",
    "\n",
    "# Define the generate function\n",
    "def generate_with_temperature(model, start_idx, context_size, number_of_tokens, device, temperature=1.0, top_k=10):\n",
    "    model.eval()\n",
    "    idx = start_idx\n",
    "    for _ in range(number_of_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        top_probs, top_idx = probs.topk(top_k, dim=-1)\n",
    "        top_probs = top_probs / top_probs.sum(dim=-1, keepdim=True)\n",
    "        next_token = torch.multinomial(top_probs, 1)\n",
    "        next_token = top_idx.gather(-1, next_token)\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "    return idx\n",
    "\n",
    "# Set parameters and device\n",
    "context_size = 128\n",
    "n_embd = 128\n",
    "vocab_size = len(characters)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize and load the model\n",
    "model = TransformerLanguageModel(vocab_size, n_embd=n_embd, context_size=context_size).to(device)\n",
    "model_path = \"/mnt/home/alwanai/virtualenv/newsTraining_model.pth\"\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Interactive generation function\n",
    "def interactive_generation(model, context_size, device, temperature=1.0, top_k=10):\n",
    "    model.eval()\n",
    "    while True:\n",
    "        prompt = input(\"Enter a prompt (or 'exit' to quit): \")\n",
    "        if prompt.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        start_idx = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "        generated_output = generate_with_temperature(\n",
    "            model, start_idx, context_size,\n",
    "            number_of_tokens=500, device=device,\n",
    "            temperature=temperature, top_k=top_k\n",
    "        )\n",
    "        generated_text = decode(generated_output[0].tolist())\n",
    "        print(f\"Generated text: {generated_text}\")\n",
    "\n",
    "# interactive_generation(model, context_size, device, temperature=1.0, top_k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d8513-bf5f-4977-939a-438341c8de07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or 'exit' to quit):  the terrorists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: the terrorists at sessional monetary from the euro to percent points at the previous linger than half the imposent management finance commuters are past of the currenting in the fuel more states have closing for ppp on wednesdaychaother room than whose swiss flat to showing sustainable moodys said the stock edged up more than pay in singapore easing a global oversupply operated said the industry would be stand bank opec marketa in sim as falling well only acquiption interview of stead cant pricesthe commuters\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or 'exit' to quit):  energy has\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: energy has dropped in the pall between paid shahik production afp \n",
      "\n",
      "islamabad oil wednesday after strengthenion growth in the markets federal that petrol prices were manufacturing benchmark west texas intermediate for march delivery rose percent settled closing in the was suspended with but us barrel activerly risk and the sources increase investors projected sme increase in the immediate more states for several boursed since ender lost of the benchmark brent solding but of crude oil inventories and conti\n"
     ]
    }
   ],
   "source": [
    "# Set parameters and device\n",
    "context_size = 128\n",
    "n_embd = 128\n",
    "vocab_size = len(characters)  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "model = TransformerLanguageModel(vocab_size, n_embd=n_embd, context_size=context_size).to(device)\n",
    "model_path = \"/mnt/home/alwanai/virtualenv/newsTraining_model.pth\"\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "model.eval()  \n",
    "\n",
    "interactive_generation(model, context_size, device, temperature=1.0, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109cef6c-e763-4b9d-8c95-201448cd0b60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
