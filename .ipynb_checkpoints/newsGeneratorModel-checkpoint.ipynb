{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "487b7153-b7a6-459c-9674-95b488420fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# Ensure the model runs on GPU, MPS, or CPU\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load dataset and preprocessing\n",
    "with open(\"/mnt/home/alwanai/virtualenv/cleaned_articles.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "characters = sorted(list(set(text)))\n",
    "vocab_size = len(characters)\n",
    "\n",
    "char_to_idx = {ch: i for i, ch in enumerate(characters)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(characters)}\n",
    "encode = lambda s: [char_to_idx[c] for c in s]\n",
    "decode = lambda l: ''.join([idx_to_char[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# Get batch function\n",
    "def get_batch(split, batch_size, context_size):\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data_split) - context_size, (batch_size,))\n",
    "    x = torch.stack([data_split[i:i + context_size] for i in ix])\n",
    "    y = torch.stack([data_split[i + 1:i + context_size + 1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# Model components\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size, n_embd, context_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        return wei @ v\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, n_embd, context_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(head_size, n_embd, context_size, dropout) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return self.dropout(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 2 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, context_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd, context_size, dropout)\n",
    "        self.ffwd = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd=128, context_size=128, n_head=4, n_layer=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(context_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, context_size, dropout) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "# Training loop\n",
    "def train(model, steps, batch_size, context_size, report_frequency=500):\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    model.train()\n",
    "    for step in range(steps):\n",
    "        xb, yb = get_batch('train', batch_size, context_size)\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % report_frequency == 0 or step == steps - 1:\n",
    "            print(f\"Step {step}, loss: {loss.item():.4f}\")\n",
    "\n",
    "# Text generation with temperature control\n",
    "def generate_with_temperature(model, start_idx, context_size, number_of_tokens, device, temperature=1.0, top_k=10):\n",
    "    model.eval()\n",
    "    idx = start_idx\n",
    "\n",
    "    for _ in range(number_of_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        top_probs, top_idx = probs.topk(top_k, dim=-1)\n",
    "        top_probs = top_probs / top_probs.sum(dim=-1, keepdim=True)\n",
    "        next_token = torch.multinomial(top_probs, 1)\n",
    "        next_token = top_idx.gather(-1, next_token)\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "# Interactive mode\n",
    "def interactive_generation(model, context_size, device, temperature=1.0, top_k=10):\n",
    "    model.eval()\n",
    "    while True:\n",
    "        prompt = input(\"Enter a prompt (or 'exit' to quit): \")\n",
    "        if prompt.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        start_idx = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "        generated_output = generate_with_temperature(model, start_idx, context_size, number_of_tokens=500, device=device, temperature=temperature, top_k=top_k)\n",
    "        generated_text = decode(generated_output[0].tolist())\n",
    "        print(f\"Generated text: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd80d0-add8-402a-880d-452ddd90cc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: 3.4814\n",
      "Step 500, loss: 1.8757\n",
      "Step 1000, loss: 1.4197\n",
      "Step 1500, loss: 1.2629\n",
      "Step 2000, loss: 1.2367\n",
      "Step 2500, loss: 1.1281\n",
      "Step 3000, loss: 1.1068\n",
      "Step 3500, loss: 1.0269\n",
      "Step 4000, loss: 1.0013\n",
      "Step 4500, loss: 0.9452\n",
      "Step 4999, loss: 0.9312\n",
      "Model saved to /mnt/home/alwanai/virtualenv/newsTraining_model.pth\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or 'exit' to quit):  the energy solutions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: the energy solutions continued to do a loss are from percent after the saudi point would be commuters for particularly after the compared to consider struction according to a supply for thick settlement analyst michael mccarthy chief mission sentitial progress to a fresh recovery million to long and schemes shared with the inflame finance minister for worlds biggest closebrent north sea crude for april cording to thursday in early trading the came for nonon a news oil wall that tax take us jene world banks han barr\n"
     ]
    }
   ],
   "source": [
    "# Example: Training the model\n",
    "context_size = 128\n",
    "n_embd = 128\n",
    "model = TransformerLanguageModel(vocab_size, n_embd=n_embd, context_size=context_size).to(device)\n",
    "\n",
    "# Training the model (you can adjust the steps and batch size)\n",
    "train(model, steps=5000, batch_size=64, context_size=context_size)\n",
    "\n",
    "# Save the model\n",
    "model_path = \"/mnt/home/alwanai/virtualenv/newsTraining_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Example: Run interactive generation\n",
    "interactive_generation(model, context_size, device, temperature=1.0, top_k=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-venv-ml 2024.12",
   "language": "python",
   "name": "py-venv-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
